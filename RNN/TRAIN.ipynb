{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "from gensim.test.utils import common_texts\n",
    "import gensim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "#load python helper functions\n",
    "from read_data import *\n",
    "from model_RNN_pytorch import GRU, LSTM\n",
    "from dataset_pytorch import *\n",
    "import importlib\n",
    "import collections\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/'\n",
    "batch_size = 250\n",
    "DIMENSION_SIZE = 200\n",
    "DATATYPE = 'OWN' # 'GLOVE' or 'OWN'\n",
    "hidden_nodes = 200\n",
    "OUTPUT_NODES = 1\n",
    "max_epochs = 100\n",
    "eval_freq = 50\n",
    "validation_set_size = 2500\n",
    "max_iter = 2500\n",
    "learning_rate = 0.001\n",
    "reached_max_iter = False\n",
    "model_type = 'LSTM'\n",
    "number_of_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "DATATYPE = DATATYPE + model_type + 'DROPOUT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wordembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove2word2vec(glove_input_file=\"./data/self_200.txt\", word2vec_output_file='./data/word2vec2.txt')\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('./data/word2vec2.txt', binary=False)\n",
    "Dataset.en_model = word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('./tensorboard/TSN-E/TWITTER/'+datetime.now().strftime(\"%Y-%m-%d %H:%M:%s\"), filename_suffix='.' + str(DIMENSION_SIZE))\n",
    "# writer.add_embedding(torch.tensor(word2vec2[all_words]), metadata=(list(all_words))) Used for t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# read dataset and get test, train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels, test_data ,test_labels = create_train_test_data(path=data_path,store_dataframe=True, pretrained=True, tokenized=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == 'LSTM':\n",
    "    print(\"Init LSTM...\")\n",
    "    model = LSTM(DIMENSION_SIZE, hidden_nodes, OUTPUT_NODES, batch_size, number_of_layers, dropout)\n",
    "else:\n",
    "    print(\"Init GRU...\")\n",
    "    model = GRU(DIMENSION_SIZE, hidden_nodes, OUTPUT_NODES, batch_size, number_of_layers, dropout)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \"\"\"Creates mini-batch tensors from the list of tuples (sequence, label).\n",
    "  \n",
    "    Args:\n",
    "        data: list of tuple (sequence, label). \n",
    "            - sequence: numpy array of shape (seq_length, 300).\n",
    "            - label:  numpy array of shape 1,\n",
    "    Returns:\n",
    "    \n",
    "    \"\"\"\n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    # unzips into sequences and labels\n",
    "    sequences, labels  = zip(*data)\n",
    "    seq_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "    sequences = [torch.from_numpy(seq) for seq in sequences]\n",
    "    # calculate sequence lengths\n",
    "    padded_sequences = pad_sequence(sequences)\n",
    "    \n",
    "    \n",
    "    return padded_sequences, torch.Tensor(labels), seq_lengths\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def accuracy(prediction, target):\n",
    "    correct = np.sum(target == prediction)\n",
    "    return correct / target.shape[0]\n",
    "\n",
    "def calculate_data_accuracy(data_generator, model, test_data=False):\n",
    "    accuracy_list, loss_list= [], []\n",
    "    if test_data:\n",
    "        length_acc = collections.defaultdict(list)\n",
    "        document_acc = []\n",
    "\n",
    "    data_counter = 0\n",
    "    model = model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in data_generator:\n",
    "            data_X, data_y, seq_length = sample\n",
    "            data_X.transpose_(0,1)\n",
    "            data_X, data_y = data_X.to(device), data_y.to(device)\n",
    "            \n",
    "            model.hidden = model.init_hidden()\n",
    "            \n",
    "            data_out = model.forward(data_X, seq_length)\n",
    "            data_out, data_y = data_out.view(data_out.numel()), data_y.view(data_y.numel())\n",
    "\n",
    "            data_loss = loss_function(data_out, data_y)\n",
    "            data_acc = accuracy(np.round(sigmoid(data_out.cpu().numpy())), data_y.cpu().numpy())\n",
    "            \n",
    "            if test_data:\n",
    "                for i, item in enumerate(np.round(sigmoid(data_out.cpu().numpy())) == data_y.cpu().numpy()):\n",
    "                    length_acc[seq_lengths[i]].append(item)\n",
    "                    document_acc.append(item)\n",
    "            \n",
    "            loss_list.append(data_loss)\n",
    "            accuracy_list.append(data_acc)\n",
    "            data_counter += 1\n",
    "    \n",
    "    accuracy_list = np.array(accuracy_list)\n",
    "    loss_list = np.array(loss_list)\n",
    "\n",
    "    if test_data:\n",
    "        return accuracy_list.sum() / data_counter, loss_list.sum() / data_counter, length_acc, np.array(document_acc)\n",
    "    else:\n",
    "        return accuracy_list.sum() / data_counter, loss_list.sum() / data_counter #fixme\n",
    "\n",
    "#fixme\n",
    "def get_dataloaders(train_data, train_labels, test_data, test_labels, val_size=500, batch_size=256):\n",
    "    # Get Dataset and use efficient dataloaders.\n",
    "    params_train = {'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0,\n",
    "              'collate_fn': collate_fn,\n",
    "              'drop_last': True}\n",
    "\n",
    "    params_validation = {'batch_size': batch_size,\n",
    "              'shuffle': True,\n",
    "              'num_workers': 0,\n",
    "              'collate_fn': collate_fn,\n",
    "              'drop_last': True}\n",
    "\n",
    "    params_test = {'batch_size': batch_size,\n",
    "              'shuffle': False,\n",
    "              'num_workers': 1,\n",
    "              'collate_fn': collate_fn,\n",
    "              'drop_last': True}\n",
    "\n",
    "\n",
    "\n",
    "    training_set = Dataset(train_data, train_labels)\n",
    "    training_set, validation_set = torch.utils.data.random_split(training_set, [len(training_set) - val_size, val_size])\n",
    "    \n",
    "    test_set = Dataset(test_data, test_labels)\n",
    "\n",
    "    training_generator = data.DataLoader(training_set, **params_train)\n",
    "    validation_generator = data.DataLoader(validation_set, **params_validation)\n",
    "    test_generator = data.DataLoader(test_set, **params_test)\n",
    "\n",
    "    return training_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataloaders\n",
    "training_generator, validation_generator, test_generator = get_dataloaders(train_data, train_labels, test_data, test_labels,\n",
    "                                                                            val_size=validation_set_size, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "loss_function = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loop over epochs\n",
    "validation_accuracy, train_accuracy = [], []\n",
    "validation_loss, train_loss = [], []\n",
    "\n",
    "iterations = 0\n",
    "train_acc = 0\n",
    "train_counter = 0\n",
    "best_acc = 0\n",
    "\n",
    "print('Starting training')\n",
    "for epoch in range(max_epochs):\n",
    "    for sample in training_generator:\n",
    "         # get the inputs\n",
    "        train_X, train_y, seq_lengths = sample\n",
    "        train_X = train_X.transpose(0,1)\n",
    "        train_X, train_y = train_X.to(device), train_y.to(device)\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Also, we need to clear out the hidden state of the LSTM,\n",
    "        # detaching it from its history on the last instance.\n",
    "        model.hidden = model.init_hidden()\n",
    "        model = model.train()\n",
    "        train_out = model.forward(train_X, seq_lengths)\n",
    "        train_out, train_y = train_out.view(train_out.numel()), train_y.view(train_y.numel())\n",
    "\n",
    "        loss = loss_function(train_out,train_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "         \n",
    "        writer.add_scalar('loss', loss.item(), iterations)\n",
    "        writer.add_scalar('accuracy', accuracy(np.round(sigmoid(train_out.cpu().detach().numpy())), train_y.cpu().detach().numpy()), iterations)\n",
    "        train_loss.append(loss.item())\n",
    "        train_accuracy.append(accuracy(np.round(sigmoid(train_out.cpu().detach().numpy())), train_y.cpu().detach().numpy()))\n",
    "         \n",
    "        iterations += 1\n",
    "        train_counter += 1\n",
    "        train_acc += train_accuracy[-1]\n",
    "        if iterations % eval_freq == 0:\n",
    "            val_acc, val_loss = calculate_data_accuracy(validation_generator, model)\n",
    "            validation_accuracy.append(val_acc)\n",
    "            validation_loss.append(val_loss)\n",
    "            \n",
    "            # Not very nice to calculate train acc and loss like this, maybe fix it later.\n",
    "            print(\"Iteration accuracy: %.2f, Train accuracy: %.2f, Validation_accuracy: %.2f, loss: %.3f \" % (iterations, (train_acc / train_counter), val_acc, np.mean(np.asarray(train_loss))))\n",
    "            train_acc = 0\n",
    "            train_counter = 0\n",
    "            if val_acc > best_acc:\n",
    "                best_acc = val_acc\n",
    "                torch.save(model.state_dict(),'./pickles/best_model_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pt')\n",
    "            \n",
    "        if iterations % max_iter == 0:\n",
    "            print('Reached maximum number of iterations')\n",
    "            reached_max_iter = True\n",
    "            break\n",
    "\n",
    "    if reached_max_iter == True:\n",
    "        break\n",
    "\n",
    "test_acc, test_loss, test_length_acc, test_document_acc = calculate_data_accuracy(test_generator, model, test_data=True)\n",
    "print('Accuracy on the test set: %.3f' % test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DUMP DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./pickles/train_accuracy_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pkl','wb') as f:\n",
    "    pickle.dump(np.array(train_accuracy), f)\n",
    "with open('./pickles/validation_accuracy_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pkl','wb') as f:\n",
    "    pickle.dump(np.array(validation_accuracy), f)\n",
    "with open('./pickles/train_loss_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pkl','wb') as f:\n",
    "    pickle.dump(np.array(train_loss), f)\n",
    "with open('./pickles/validation_loss_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pkl','wb') as f:\n",
    "    pickle.dump(np.array(validation_loss), f)\n",
    "    \n",
    "with open('./pickles/val_length_acc_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pkl','wb') as f:\n",
    "    pickle.dump(test_length_acc, f)\n",
    "with open('./pickles/val_document_acc_'+str(DIMENSION_SIZE)+'_'+str(DATATYPE)+'.pkl','wb') as f:\n",
    "    pickle.dump(test_document_acc, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
